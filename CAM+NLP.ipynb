{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAM+NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbcSJggs6W1G",
        "colab_type": "code",
        "outputId": "210714ce-3d07-4206-80a0-5784a768e9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#downloading library\n",
        "!pip install pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.16.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->pytorch-nlp) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLIvXMb26Wt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Defining dataset extraction function\"\"\"\n",
        "import os\n",
        "import io\n",
        "\n",
        "import json\n",
        "\n",
        "from torchnlp.download import download_file_maybe_extract\n",
        "from torchnlp.datasets.dataset import Dataset\n",
        "\n",
        "def snli_dataset(directory='data/',\n",
        "                 train=False,\n",
        "                 dev=False,\n",
        "                 test=False,\n",
        "                 train_filename='snli_1.0_train.jsonl',\n",
        "                 dev_filename='snli_1.0_dev.jsonl',\n",
        "                 test_filename='snli_1.0_test.jsonl',\n",
        "                 extracted_name='snli_1.0',\n",
        "                 check_files=['snli_1.0/snli_1.0_train.jsonl'],\n",
        "                 url='http://nlp.stanford.edu/projects/snli/snli_1.0.zip'):\n",
        "    \n",
        "    \"\"\"Load the Stanford Natural Language Inference (SNLI) dataset.\n",
        "\n",
        "    The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs\n",
        "    manually labeled for balanced classification with the labels entailment, contradiction, and\n",
        "    neutral, supporting the task of natural language inference (NLI), also known as recognizing\n",
        "    textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating\n",
        "    representational systems for text, especially including those induced by representation\n",
        "    learning methods, as well as a resource for developing NLP models of any kind.\n",
        "\n",
        "    **Reference:** https://nlp.stanford.edu/projects/snli/\n",
        "\n",
        "    Args:\n",
        "        directory (str, optional): Directory to cache the dataset.\n",
        "        train (bool, optional): If to load the training split of the dataset.\n",
        "        dev (bool, optional): If to load the development split of the dataset.\n",
        "        test (bool, optional): If to load the test split of the dataset.\n",
        "        train_filename (str, optional): The filename of the training split.\n",
        "        dev_filename (str, optional): The filename of the development split.\n",
        "        test_filename (str, optional): The filename of the test split.\n",
        "        extracted_name (str, optional): Name of the extracted dataset directory.\n",
        "        check_files (str, optional): Check if these files exist, then this download was successful.\n",
        "        url (str, optional): URL of the dataset `tar.gz` file.\n",
        "\n",
        "    Returns:\n",
        "        :class:`tuple` of :class:`torchnlp.datasets.Dataset` or :class:`torchnlp.datasets.Dataset`:\n",
        "        Returns between one and all dataset splits (train, dev and test) depending on if their\n",
        "        respective boolean argument is ``True``.\n",
        "\n",
        "        }\n",
        "    \"\"\"\n",
        "    download_file_maybe_extract(url=url, directory=directory, check_files=check_files)\n",
        "    ret = []\n",
        "    splits = [(train, train_filename), (dev, dev_filename), (test, test_filename)]\n",
        "    splits = [f for (requested, f) in splits if requested]\n",
        "    for filename in splits:\n",
        "        full_path = os.path.join(directory, extracted_name, filename)\n",
        "        examples = []\n",
        "        with io.open(full_path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = json.loads(line)\n",
        "                examples.append({\n",
        "                    'premise': line['sentence1'],\n",
        "                    'hypothesis': line['sentence2'],\n",
        "                    'label': line['gold_label']\n",
        "                })\n",
        "        ret.append(Dataset(examples))\n",
        "\n",
        "    if len(ret) == 1:\n",
        "        return ret[0]\n",
        "    else:\n",
        "        return tuple(ret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBtcHofq6W8z",
        "colab_type": "code",
        "outputId": "e634c880-0f47-4029-9482-52e1f3693fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#Obtaining dataset\n",
        "train=snli_dataset(train=True) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': 'A team is trying to tag a runner out.',\n",
              " 'label': 'entailment',\n",
              " 'premise': 'A Little League team tries to catch a runner sliding into a base in an afternoon game.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0aHPcV1U3Yz",
        "colab_type": "code",
        "outputId": "1d4e4f71-330c-4fb5-8ae0-7efab28cb7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#viewing training examples\n",
        "train[36] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': 'The women do not care what clothes they wear.',\n",
              " 'label': 'contradiction',\n",
              " 'premise': 'High fashion ladies wait outside a tram beside a crowd of people in the city.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syFrI3NjdcF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Importing required libraries\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.padding import ConstantPad1d\n",
        "from torch import topk \n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "# %matplotlib inline\n",
        "from matplotlib.pyplot import imshow\n",
        "from torchvision import models, transforms\n",
        "import skimage.transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0LR08AocSa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Defining class CNN consisting of the model architecture to be used\"\"\"\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,vocab_size,embedding_length,weights):\n",
        "        super(CNN, self).__init__()\n",
        "        self.word_embeddings=nn.Embedding(vocab_size,embedding_length)\n",
        "        self.word_embeddings.weight=nn.Parameter(weights,requires_grad=False)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=10, stride=2)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=1, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=100, kernel_size=5, stride=2)\n",
        "        self.AdapAvgPool=nn.AdaptiveAvgPool2d(((1,1)))\n",
        "        self.fc1 = nn.Linear(in_features=100, out_features=3)\n",
        "        \n",
        "    def forward(self,input_sentences):\n",
        "        x=self.word_embeddings(input_sentences)\n",
        "        x=x.unsqueeze(1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x =self.AdapAvgPool((F.relu(self.conv2(x))))\n",
        "        x = x.view(-1, 1*1*100)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ytYJISdl2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Function to load training and test dataset as iterators\"\"\"\n",
        "def load_dataset(test_sen=None):\n",
        "  tokenize=lambda x:x.split()\n",
        "  Text=data.Field(sequential=True,tokenize=tokenize, lower=True, include_lengths=True,batch_first=True,fix_length=50)\n",
        "  label=data.LabelField()\n",
        "  train_data,valid_data,test_data=datasets.SNLI.splits(Text,label)\n",
        "  Text.build_vocab(train_data,vectors=GloVe(name='6B',dim=100))\n",
        "  label.build_vocab(train_data)\n",
        "  \n",
        "  word_embeddings=Text.vocab.vectors\n",
        "  print(\"Length of text vocab\"+str(len(Text.vocab)))\n",
        "  print(\"Vector size of text vocab\",Text.vocab.vectors.size())\n",
        "  print(\"Label Length:\"+str(len(label.vocab)))\n",
        "  \n",
        "  #train_data,valid_data=train_data.split()\n",
        "  train_iter,valid_iter,test_iter=data.BucketIterator.splits((train_data,valid_data,test_data),batch_size=32,sort_key=lambda x:len(x.premise),repeat=False,shuffle=True)\n",
        "  \n",
        "  \n",
        "  vocab_size=len(Text.vocab)\n",
        "  \n",
        "  return Text,vocab_size,word_embeddings,train_iter,valid_iter,test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVUw2Giedl5z",
        "colab_type": "code",
        "outputId": "07873bba-123c-4bc3-b793-52d825a3e7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "\"\"\"Loading dataset into variables\"\"\"\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter,test_iter = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading snli_1.0.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:19<00:00, 2.65MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:24, 35.0MB/s]                           \n",
            "100%|█████████▉| 399454/400000 [00:16<00:00, 25212.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of text vocab56220\n",
            "Vector size of text vocab torch.Size([56220, 100])\n",
            "Label Length:3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fNVe1eWzhZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model,clip_value):\n",
        "  params=list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "  for p in params:\n",
        "    p.grad.data.clamp(-clip_value,clip_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwYBOGe3siPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Function to train our model and output the accuracy\"\"\"\n",
        "def train_model(model,train_iter,epoch):\n",
        "  total_epoch_loss=0\n",
        "  total_epoch_acc=0\n",
        "\n",
        "  model.cuda()\n",
        "  optim=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "  steps=0\n",
        "  model.train()\n",
        "  for idx,batch in enumerate(train_iter):\n",
        "    premise=batch.premise[0]\n",
        "    hypothesis=batch.hypothesis[0]\n",
        "    target=batch.label\n",
        "    target=torch.autograd.Variable(target).long()\n",
        "    if torch.cuda.is_available():\n",
        "      premise=premise.cuda()\n",
        "      hypothesis=hypothesis.cuda()\n",
        "      target=target.cuda()   \n",
        "#if (premise.size()[0] is not 50 or hypothesis.size()[0] is not 50):continue\n",
        "    optim.zero_grad()\n",
        "    joint=torch.cat([premise,hypothesis],1)\n",
        "    prediction=model(joint)\n",
        "    loss=loss_fn(prediction,target)\n",
        "    num_corrects=(torch.max(prediction,1)[1].view(target.size()).data==target.data).float().sum()\n",
        "    acc=100.0*num_corrects/len(batch)\n",
        "    loss.backward()\n",
        "    clip_gradient(model,1e-1)\n",
        "    optim.step()\n",
        "    steps+=1\n",
        "    if steps%5000==0:\n",
        "      print(f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item(): .4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "    total_epoch_loss+=loss.item()\n",
        "    total_epoch_acc+=acc.item()\n",
        "  return total_epoch_loss/len(train_iter),total_epoch_acc/len(train_iter)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOg5j5jdsiWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Function to evaluate our test set using our model and thus output the accuracy\"\"\"\n",
        "def eval_model(model, val_iter):\n",
        "  total_epoch_loss = 0\n",
        "  total_epoch_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for idx, batch in enumerate(val_iter):\n",
        "          premise=batch.premise[0]\n",
        "          hypothesis=batch.hypothesis[0]\n",
        "          if (premise.size()[0] is not 32 or hypothesis.size()[0] is not 32):\n",
        "            continue\n",
        "          target = batch.label\n",
        "          target = torch.autograd.Variable(target).long()\n",
        "          if torch.cuda.is_available():\n",
        "              premise=premise.cuda()\n",
        "              hypothesis=hypothesis.cuda()\n",
        "              target = target.cuda()\n",
        "          joint=torch.cat([premise,hypothesis],1)\n",
        "          prediction=model(joint)\n",
        "          loss = loss_fn(prediction, target)\n",
        "          num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "          acc = 100.0 * num_corrects/len(batch)\n",
        "          total_epoch_loss += loss.item()\n",
        "          total_epoch_acc += acc.item()\n",
        "  return total_epoch_loss/len(val_iter),total_epoch_acc/len(val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc_IsG3w9CVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Saving our model into a variable and defining our loss function\"\"\"\n",
        "model=CNN(vocab_size=56220,embedding_length=100,weights=word_embeddings)\n",
        "loss_fn=F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bedzgKHpsvJW",
        "colab_type": "code",
        "outputId": "44dd011d-7e29-4c61-988f-320f82ddf4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2813
        }
      },
      "source": [
        "\"\"\"Running our training for X epochs and outputing test accuracy\"\"\"\n",
        "for epoch in range(40):\n",
        "  train_loss,train_acc=train_model(model,train_iter, epoch)\n",
        "  val_loss,val_acc=eval_model(model,valid_iter)\n",
        "  print(f'Epoch:{epoch+1:02},Train Loss:{train_loss:.3f},Train Acc:{train_acc:.2f}%,Val.Loss:{val_loss:3f},Val.Acc:{val_acc:.2f}%')\n",
        "test_loss,test_acc=eval_model(model,test_iter)\n",
        "print(f'Test Loss:{test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Idx: 5000, Training Loss:  0.9385, Training Accuracy:  62.50%\n",
            "Epoch: 1, Idx: 10000, Training Loss:  0.9065, Training Accuracy:  53.12%\n",
            "Epoch: 1, Idx: 15000, Training Loss:  0.9735, Training Accuracy:  50.00%\n",
            "Epoch:01,Train Loss:0.974,Train Acc:52.35%,Val.Loss:0.929620,Val.Acc:55.20%\n",
            "Epoch: 2, Idx: 5000, Training Loss:  0.8988, Training Accuracy:  65.62%\n",
            "Epoch: 2, Idx: 10000, Training Loss:  0.9410, Training Accuracy:  53.12%\n",
            "Epoch: 2, Idx: 15000, Training Loss:  0.8425, Training Accuracy:  56.25%\n",
            "Epoch:02,Train Loss:0.929,Train Acc:55.85%,Val.Loss:0.913131,Val.Acc:56.37%\n",
            "Epoch: 3, Idx: 5000, Training Loss:  0.9418, Training Accuracy:  50.00%\n",
            "Epoch: 3, Idx: 10000, Training Loss:  0.9478, Training Accuracy:  53.12%\n",
            "Epoch: 3, Idx: 15000, Training Loss:  0.8961, Training Accuracy:  56.25%\n",
            "Epoch:03,Train Loss:0.912,Train Acc:56.99%,Val.Loss:0.892832,Val.Acc:57.91%\n",
            "Epoch: 4, Idx: 5000, Training Loss:  0.8263, Training Accuracy:  59.38%\n",
            "Epoch: 4, Idx: 10000, Training Loss:  1.0725, Training Accuracy:  46.88%\n",
            "Epoch: 4, Idx: 15000, Training Loss:  0.9462, Training Accuracy:  59.38%\n",
            "Epoch:04,Train Loss:0.899,Train Acc:57.89%,Val.Loss:0.889760,Val.Acc:57.70%\n",
            "Epoch: 5, Idx: 5000, Training Loss:  0.9988, Training Accuracy:  46.88%\n",
            "Epoch: 5, Idx: 10000, Training Loss:  0.9791, Training Accuracy:  40.62%\n",
            "Epoch: 5, Idx: 15000, Training Loss:  0.9026, Training Accuracy:  62.50%\n",
            "Epoch:05,Train Loss:0.888,Train Acc:58.57%,Val.Loss:0.883568,Val.Acc:57.97%\n",
            "Epoch: 6, Idx: 5000, Training Loss:  0.8916, Training Accuracy:  62.50%\n",
            "Epoch: 6, Idx: 10000, Training Loss:  0.9112, Training Accuracy:  59.38%\n",
            "Epoch: 6, Idx: 15000, Training Loss:  0.8103, Training Accuracy:  59.38%\n",
            "Epoch:06,Train Loss:0.879,Train Acc:59.15%,Val.Loss:0.890696,Val.Acc:57.81%\n",
            "Epoch: 7, Idx: 5000, Training Loss:  0.9708, Training Accuracy:  56.25%\n",
            "Epoch: 7, Idx: 10000, Training Loss:  0.8017, Training Accuracy:  62.50%\n",
            "Epoch: 7, Idx: 15000, Training Loss:  0.9224, Training Accuracy:  59.38%\n",
            "Epoch:07,Train Loss:0.872,Train Acc:59.67%,Val.Loss:0.872886,Val.Acc:58.60%\n",
            "Epoch: 8, Idx: 5000, Training Loss:  0.9256, Training Accuracy:  59.38%\n",
            "Epoch: 8, Idx: 10000, Training Loss:  0.7771, Training Accuracy:  50.00%\n",
            "Epoch: 8, Idx: 15000, Training Loss:  0.8124, Training Accuracy:  71.88%\n",
            "Epoch:08,Train Loss:0.864,Train Acc:60.22%,Val.Loss:0.876070,Val.Acc:58.79%\n",
            "Epoch: 9, Idx: 5000, Training Loss:  0.9249, Training Accuracy:  53.12%\n",
            "Epoch: 9, Idx: 10000, Training Loss:  1.0337, Training Accuracy:  46.88%\n",
            "Epoch: 9, Idx: 15000, Training Loss:  0.8366, Training Accuracy:  62.50%\n",
            "Epoch:09,Train Loss:0.858,Train Acc:60.57%,Val.Loss:0.875842,Val.Acc:58.89%\n",
            "Epoch: 10, Idx: 5000, Training Loss:  0.9199, Training Accuracy:  56.25%\n",
            "Epoch: 10, Idx: 10000, Training Loss:  0.8983, Training Accuracy:  59.38%\n",
            "Epoch: 10, Idx: 15000, Training Loss:  0.7894, Training Accuracy:  71.88%\n",
            "Epoch:10,Train Loss:0.852,Train Acc:60.94%,Val.Loss:0.875774,Val.Acc:58.77%\n",
            "Epoch: 11, Idx: 5000, Training Loss:  0.9466, Training Accuracy:  50.00%\n",
            "Epoch: 11, Idx: 10000, Training Loss:  0.8151, Training Accuracy:  62.50%\n",
            "Epoch: 11, Idx: 15000, Training Loss:  0.8886, Training Accuracy:  56.25%\n",
            "Epoch:11,Train Loss:0.847,Train Acc:61.31%,Val.Loss:0.877837,Val.Acc:58.56%\n",
            "Epoch: 12, Idx: 5000, Training Loss:  0.7995, Training Accuracy:  62.50%\n",
            "Epoch: 12, Idx: 10000, Training Loss:  0.8152, Training Accuracy:  53.12%\n",
            "Epoch: 12, Idx: 15000, Training Loss:  0.8062, Training Accuracy:  59.38%\n",
            "Epoch:12,Train Loss:0.841,Train Acc:61.61%,Val.Loss:0.872108,Val.Acc:58.73%\n",
            "Epoch: 13, Idx: 5000, Training Loss:  0.7427, Training Accuracy:  71.88%\n",
            "Epoch: 13, Idx: 10000, Training Loss:  0.7463, Training Accuracy:  71.88%\n",
            "Epoch: 13, Idx: 15000, Training Loss:  0.8076, Training Accuracy:  62.50%\n",
            "Epoch:13,Train Loss:0.836,Train Acc:61.91%,Val.Loss:0.877278,Val.Acc:58.34%\n",
            "Epoch: 14, Idx: 5000, Training Loss:  0.8316, Training Accuracy:  62.50%\n",
            "Epoch: 14, Idx: 10000, Training Loss:  0.8940, Training Accuracy:  65.62%\n",
            "Epoch: 14, Idx: 15000, Training Loss:  0.7398, Training Accuracy:  65.62%\n",
            "Epoch:14,Train Loss:0.831,Train Acc:62.22%,Val.Loss:0.871784,Val.Acc:59.19%\n",
            "Epoch: 15, Idx: 5000, Training Loss:  0.8084, Training Accuracy:  59.38%\n",
            "Epoch: 15, Idx: 10000, Training Loss:  0.7199, Training Accuracy:  71.88%\n",
            "Epoch: 15, Idx: 15000, Training Loss:  0.7802, Training Accuracy:  65.62%\n",
            "Epoch:15,Train Loss:0.827,Train Acc:62.54%,Val.Loss:0.879534,Val.Acc:58.64%\n",
            "Epoch: 16, Idx: 5000, Training Loss:  0.8106, Training Accuracy:  71.88%\n",
            "Epoch: 16, Idx: 10000, Training Loss:  0.8770, Training Accuracy:  59.38%\n",
            "Epoch: 16, Idx: 15000, Training Loss:  0.9825, Training Accuracy:  53.12%\n",
            "Epoch:16,Train Loss:0.823,Train Acc:62.76%,Val.Loss:0.876219,Val.Acc:58.93%\n",
            "Epoch: 17, Idx: 5000, Training Loss:  0.7772, Training Accuracy:  62.50%\n",
            "Epoch: 17, Idx: 10000, Training Loss:  0.6840, Training Accuracy:  68.75%\n",
            "Epoch: 17, Idx: 15000, Training Loss:  0.7418, Training Accuracy:  78.12%\n",
            "Epoch:17,Train Loss:0.819,Train Acc:62.97%,Val.Loss:0.877336,Val.Acc:58.96%\n",
            "Epoch: 18, Idx: 5000, Training Loss:  0.7190, Training Accuracy:  59.38%\n",
            "Epoch: 18, Idx: 10000, Training Loss:  0.8075, Training Accuracy:  62.50%\n",
            "Epoch: 18, Idx: 15000, Training Loss:  0.8961, Training Accuracy:  50.00%\n",
            "Epoch:18,Train Loss:0.815,Train Acc:63.17%,Val.Loss:0.877809,Val.Acc:58.28%\n",
            "Epoch: 19, Idx: 5000, Training Loss:  0.7529, Training Accuracy:  68.75%\n",
            "Epoch: 19, Idx: 10000, Training Loss:  0.8869, Training Accuracy:  56.25%\n",
            "Epoch: 19, Idx: 15000, Training Loss:  0.8524, Training Accuracy:  65.62%\n",
            "Epoch:19,Train Loss:0.812,Train Acc:63.43%,Val.Loss:0.887179,Val.Acc:58.44%\n",
            "Epoch: 20, Idx: 5000, Training Loss:  0.6822, Training Accuracy:  68.75%\n",
            "Epoch: 20, Idx: 10000, Training Loss:  0.8184, Training Accuracy:  71.88%\n",
            "Epoch: 20, Idx: 15000, Training Loss:  0.8831, Training Accuracy:  59.38%\n",
            "Epoch:20,Train Loss:0.808,Train Acc:63.66%,Val.Loss:0.882056,Val.Acc:58.75%\n",
            "Epoch: 21, Idx: 5000, Training Loss:  0.7164, Training Accuracy:  71.88%\n",
            "Epoch: 21, Idx: 10000, Training Loss:  0.7923, Training Accuracy:  65.62%\n",
            "Epoch: 21, Idx: 15000, Training Loss:  0.7631, Training Accuracy:  65.62%\n",
            "Epoch:21,Train Loss:0.805,Train Acc:63.81%,Val.Loss:0.884702,Val.Acc:58.88%\n",
            "Epoch: 22, Idx: 5000, Training Loss:  0.8302, Training Accuracy:  62.50%\n",
            "Epoch: 22, Idx: 10000, Training Loss:  0.7947, Training Accuracy:  65.62%\n",
            "Epoch: 22, Idx: 15000, Training Loss:  0.7648, Training Accuracy:  65.62%\n",
            "Epoch:22,Train Loss:0.802,Train Acc:63.98%,Val.Loss:0.899858,Val.Acc:58.01%\n",
            "Epoch: 23, Idx: 5000, Training Loss:  0.8241, Training Accuracy:  59.38%\n",
            "Epoch: 23, Idx: 10000, Training Loss:  0.8983, Training Accuracy:  59.38%\n",
            "Epoch: 23, Idx: 15000, Training Loss:  0.7981, Training Accuracy:  65.62%\n",
            "Epoch:23,Train Loss:0.799,Train Acc:64.19%,Val.Loss:0.891212,Val.Acc:58.87%\n",
            "Epoch: 24, Idx: 5000, Training Loss:  0.7533, Training Accuracy:  68.75%\n",
            "Epoch: 24, Idx: 10000, Training Loss:  0.7682, Training Accuracy:  65.62%\n",
            "Epoch: 24, Idx: 15000, Training Loss:  0.7468, Training Accuracy:  68.75%\n",
            "Epoch:24,Train Loss:0.796,Train Acc:64.29%,Val.Loss:0.893162,Val.Acc:58.36%\n",
            "Epoch: 25, Idx: 5000, Training Loss:  0.7817, Training Accuracy:  62.50%\n",
            "Epoch: 25, Idx: 10000, Training Loss:  0.7804, Training Accuracy:  68.75%\n",
            "Epoch: 25, Idx: 15000, Training Loss:  0.7289, Training Accuracy:  75.00%\n",
            "Epoch:25,Train Loss:0.793,Train Acc:64.52%,Val.Loss:0.892853,Val.Acc:58.69%\n",
            "Epoch: 26, Idx: 5000, Training Loss:  0.6541, Training Accuracy:  68.75%\n",
            "Epoch: 26, Idx: 10000, Training Loss:  0.9282, Training Accuracy:  62.50%\n",
            "Epoch: 26, Idx: 15000, Training Loss:  0.8043, Training Accuracy:  62.50%\n",
            "Epoch:26,Train Loss:0.790,Train Acc:64.61%,Val.Loss:0.896071,Val.Acc:58.69%\n",
            "Epoch: 27, Idx: 5000, Training Loss:  0.5584, Training Accuracy:  78.12%\n",
            "Epoch: 27, Idx: 10000, Training Loss:  0.8514, Training Accuracy:  65.62%\n",
            "Epoch: 27, Idx: 15000, Training Loss:  0.7417, Training Accuracy:  65.62%\n",
            "Epoch:27,Train Loss:0.788,Train Acc:64.86%,Val.Loss:0.914045,Val.Acc:58.61%\n",
            "Epoch: 28, Idx: 5000, Training Loss:  0.7214, Training Accuracy:  68.75%\n",
            "Epoch: 28, Idx: 10000, Training Loss:  1.0309, Training Accuracy:  53.12%\n",
            "Epoch: 28, Idx: 15000, Training Loss:  0.7395, Training Accuracy:  71.88%\n",
            "Epoch:28,Train Loss:0.785,Train Acc:64.96%,Val.Loss:0.899989,Val.Acc:58.41%\n",
            "Epoch: 29, Idx: 5000, Training Loss:  0.9221, Training Accuracy:  59.38%\n",
            "Epoch: 29, Idx: 10000, Training Loss:  0.8213, Training Accuracy:  59.38%\n",
            "Epoch: 29, Idx: 15000, Training Loss:  0.8032, Training Accuracy:  68.75%\n",
            "Epoch:29,Train Loss:0.783,Train Acc:65.09%,Val.Loss:0.903948,Val.Acc:58.05%\n",
            "Epoch: 30, Idx: 5000, Training Loss:  0.9104, Training Accuracy:  68.75%\n",
            "Epoch: 30, Idx: 10000, Training Loss:  0.8525, Training Accuracy:  59.38%\n",
            "Epoch: 30, Idx: 15000, Training Loss:  0.6980, Training Accuracy:  71.88%\n",
            "Epoch:30,Train Loss:0.781,Train Acc:65.24%,Val.Loss:0.901124,Val.Acc:58.43%\n",
            "Epoch: 31, Idx: 5000, Training Loss:  0.8225, Training Accuracy:  62.50%\n",
            "Epoch: 31, Idx: 10000, Training Loss:  0.8640, Training Accuracy:  59.38%\n",
            "Epoch: 31, Idx: 15000, Training Loss:  0.7055, Training Accuracy:  68.75%\n",
            "Epoch:31,Train Loss:0.778,Train Acc:65.35%,Val.Loss:0.906120,Val.Acc:58.18%\n",
            "Epoch: 32, Idx: 5000, Training Loss:  0.5292, Training Accuracy:  81.25%\n",
            "Epoch: 32, Idx: 10000, Training Loss:  0.6771, Training Accuracy:  71.88%\n",
            "Epoch: 32, Idx: 15000, Training Loss:  0.6330, Training Accuracy:  81.25%\n",
            "Epoch:32,Train Loss:0.776,Train Acc:65.51%,Val.Loss:0.908093,Val.Acc:58.21%\n",
            "Epoch: 33, Idx: 5000, Training Loss:  0.6183, Training Accuracy:  71.88%\n",
            "Epoch: 33, Idx: 10000, Training Loss:  0.7957, Training Accuracy:  78.12%\n",
            "Epoch: 33, Idx: 15000, Training Loss:  0.8153, Training Accuracy:  65.62%\n",
            "Epoch:33,Train Loss:0.774,Train Acc:65.63%,Val.Loss:0.911480,Val.Acc:58.23%\n",
            "Epoch: 34, Idx: 5000, Training Loss:  0.6368, Training Accuracy:  75.00%\n",
            "Epoch: 34, Idx: 10000, Training Loss:  0.8182, Training Accuracy:  62.50%\n",
            "Epoch: 34, Idx: 15000, Training Loss:  0.6655, Training Accuracy:  78.12%\n",
            "Epoch:34,Train Loss:0.772,Train Acc:65.73%,Val.Loss:0.920943,Val.Acc:57.69%\n",
            "Epoch: 35, Idx: 5000, Training Loss:  0.5758, Training Accuracy:  81.25%\n",
            "Epoch: 35, Idx: 10000, Training Loss:  0.8168, Training Accuracy:  71.88%\n",
            "Epoch: 35, Idx: 15000, Training Loss:  0.7888, Training Accuracy:  78.12%\n",
            "Epoch:35,Train Loss:0.770,Train Acc:65.84%,Val.Loss:0.918319,Val.Acc:58.07%\n",
            "Epoch: 36, Idx: 5000, Training Loss:  0.8354, Training Accuracy:  59.38%\n",
            "Epoch: 36, Idx: 10000, Training Loss:  0.7696, Training Accuracy:  62.50%\n",
            "Epoch: 36, Idx: 15000, Training Loss:  0.8133, Training Accuracy:  68.75%\n",
            "Epoch:36,Train Loss:0.768,Train Acc:65.97%,Val.Loss:0.918781,Val.Acc:58.22%\n",
            "Epoch: 37, Idx: 5000, Training Loss:  0.7566, Training Accuracy:  71.88%\n",
            "Epoch: 37, Idx: 10000, Training Loss:  0.7221, Training Accuracy:  71.88%\n",
            "Epoch: 37, Idx: 15000, Training Loss:  0.8115, Training Accuracy:  68.75%\n",
            "Epoch:37,Train Loss:0.766,Train Acc:66.02%,Val.Loss:0.920569,Val.Acc:57.94%\n",
            "Epoch: 38, Idx: 5000, Training Loss:  0.6275, Training Accuracy:  75.00%\n",
            "Epoch: 38, Idx: 10000, Training Loss:  0.8208, Training Accuracy:  68.75%\n",
            "Epoch: 38, Idx: 15000, Training Loss:  0.7582, Training Accuracy:  68.75%\n",
            "Epoch:38,Train Loss:0.765,Train Acc:66.13%,Val.Loss:0.921256,Val.Acc:57.51%\n",
            "Epoch: 39, Idx: 5000, Training Loss:  0.5785, Training Accuracy:  78.12%\n",
            "Epoch: 39, Idx: 10000, Training Loss:  0.5399, Training Accuracy:  75.00%\n",
            "Epoch: 39, Idx: 15000, Training Loss:  0.8160, Training Accuracy:  56.25%\n",
            "Epoch:39,Train Loss:0.763,Train Acc:66.23%,Val.Loss:0.935922,Val.Acc:57.69%\n",
            "Epoch: 40, Idx: 5000, Training Loss:  0.8703, Training Accuracy:  65.62%\n",
            "Epoch: 40, Idx: 10000, Training Loss:  0.6791, Training Accuracy:  71.88%\n",
            "Epoch: 40, Idx: 15000, Training Loss:  0.6596, Training Accuracy:  81.25%\n",
            "Epoch:40,Train Loss:0.761,Train Acc:66.33%,Val.Loss:0.938711,Val.Acc:57.13%\n",
            "Test Loss:0.950, Test Acc: 57.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg5cF5RrNqCh",
        "colab_type": "code",
        "outputId": "e44e50d2-78e0-4218-8584-1c4781985693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "''' Let us now predict the class on a single sentence just for the testing purpose. '''\n",
        "test_sen1 = \"Nobody goes to that restaurant.\"\n",
        "test_sen2 = \"It is too crowded.\"\n",
        "\n",
        "test_sen1 = TEXT.preprocess(test_sen1)\n",
        "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "test_sen2 = TEXT.preprocess(test_sen2)\n",
        "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n",
        "premise_sen = np.asarray(test_sen1)\n",
        "premise_sen = torch.LongTensor(premise_sen)\n",
        "premise_tensor = Variable(premise_sen, volatile=True)\n",
        "hypo_sen = np.asarray(test_sen2)\n",
        "hypo_sen = torch.LongTensor(hypo_sen)\n",
        "hypo_tensor = Variable(hypo_sen, volatile=True)\n",
        "size1=premise_tensor[0].size()[0]\n",
        "m=nn.ConstantPad1d((0,50-size1),1)\n",
        "prem_tensor=m(premise_tensor)\n",
        "size2=hypo_tensor[0].size()[0]\n",
        "m=nn.ConstantPad1d((0,50-size2),1)\n",
        "hyp_tensor=m(hypo_tensor)\n",
        "test_tensor=torch.cat([prem_tensor,hyp_tensor],1)\n",
        "test_tensor = test_tensor.cuda()\n",
        "model.eval()\n",
        "output = model(test_tensor)\n",
        "out=F.softmax(output,1)\n",
        "print(out)\n",
        "if(torch.argmax(out[0])==0):\n",
        "  print(\"Entailment\")\n",
        "elif(torch.argmax(out[0])==1):\n",
        "  print(\"Contradiction\")\n",
        "else:\n",
        "  print(\"Neutral\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5776, 0.0649, 0.3576]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "Entailment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ_fiTnWU_tr",
        "colab_type": "code",
        "outputId": "793b018b-1a8e-466b-9586-538abc7e4f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "\"\"\"Viewing the architecture of our model\"\"\"\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (word_embeddings): Embedding(56220, 100)\n",
              "  (conv1): Conv2d(1, 50, kernel_size=(10, 10), stride=(2, 2))\n",
              "  (pool): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
              "  (conv2): Conv2d(50, 100, kernel_size=(5, 5), stride=(2, 2))\n",
              "  (AdapAvgPool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc1): Linear(in_features=100, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH17_kGxcfEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Creating a hook and saving particular layer features\"\"\"\n",
        "class SaveFeatures():\n",
        "  features=None\n",
        "  def __init__(self,m):self.hook=m.register_forward_hook(self.hook_fn)\n",
        "  def hook_fn(self,module,input,output): self.features=((output.cpu()).data).numpy()\n",
        "  def remove(self): self.hook.remove()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTXmAlDKce8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Obtaining the layer needed\"\"\"\n",
        "final_layer=model._modules.get('conv2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FXBJWkqce5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Saving the feature of the layer needed\"\"\"\n",
        "activated_features=SaveFeatures(final_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUAoYGhpce37",
        "colab_type": "code",
        "outputId": "306a0628-5353-4d2b-fe3b-2a481fe0823e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\"\"\"Making the prediction and obtaining the layer features in that case\"\"\"\n",
        "prediction=model(test_tensor)\n",
        "pred_probabilities=F.softmax(prediction).data.squeeze()\n",
        "activated_features.remove()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4eK1gdzce1_",
        "colab_type": "code",
        "outputId": "8fe3cfb2-82d0-42dd-a329-00ca9f6ff2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "topk(pred_probabilities,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([0.8476], device='cuda:0'), indices=tensor([2], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTAQsfqHceyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Obatining our final heatmap from the features by dot producting the weights and activation maps\"\"\"\n",
        "def getCAM(feature_conv, weight_fc, class_idx):\n",
        "  _,nc,h,w=feature_conv.shape\n",
        "  cam=weight_fc[class_idx.cpu()].dot(feature_conv.reshape(((nc,h*w))))\n",
        "  cam=cam.reshape(h,w)\n",
        "  #cam=cam-np.min(cam)\n",
        "  #cam_img=cam/np.max(cam)\n",
        "  return [cam]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nClocPJwceu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_softmax_params=list(model._modules.get('fc1').parameters())\n",
        "weight_softmax=np.squeeze(weight_softmax_params[0].cpu().data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy34u_TnlSm0",
        "colab_type": "code",
        "outputId": "2a79a1a4-6ee1-4bcc-d132-98751e073786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "activated_features.features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 100, 10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6dTCIL2lSQ4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-zzns2cere",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_idx=topk(pred_probabilities,1)[1].int()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiaD4_DHc8Q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overlay=getCAM(activated_features.features, weight_softmax, class_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wzrq5AJc8MF",
        "colab_type": "code",
        "outputId": "e692c1a2-f5de-4bd6-dd2e-251c590b8653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "\"\"\"Plotting the map obtained\"\"\"\n",
        "imshow(overlay[0], alpha=0.4, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed0eb800f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByAOzC53c8KF",
        "colab_type": "code",
        "outputId": "a5de2887-f872-47ba-b999-55d47ee75f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "\"\"\"Plotting the similar heatmap resized to 100*100\"\"\"\n",
        "imshow(skimage.transform.resize(overlay[0], [100,100]),alpha=0.6, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fed0d4881d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnVusJMd53//fXM5lz164vHi9JmWT\ngQQFhAFHwkKRoCAwRBtRFME0DMGQZQiEQYMvTkw7BmzKebADOIAFGJb1EAhYhDGIQAjj0EIoyIYN\nhaYf8sJoaQm2RUqmIsUmGVKkbHKX5O6eM2em8tD1zVR/U1VdPTNn5iz7/wMGPV1V3V19qf5/9dWl\nxTkHQki36G06A4SQ9cOCT0gHYcEnpIOw4BPSQVjwCekgLPiEdBAWfEI6yFIFX0Q+LCLfFJFvichD\nq8oUIeRokUU78IhIH8DfAPhxAC8A+AqAn3HOPbO67BFCjoLBEtu+D8C3nHPfBgAReRTAvQCSBX9n\n51Z36tSd+b2KWWfHwvlrsqrtcvGpuFh40z0ruYexNE3bLbrf4o3ssm0axV6gpvVUWOz49bDvfe+5\n6869tduw8VIF/3YAzwfrLwD4pzaRiDwA4AEAOHnyB/FTP3Upv1db+ZgskcMoy7xJSjJjT2DRUpvZ\nZUh/ge00vJ9JZ+PsNqkwABj75cSsh0wSacJLbLezlz92O5q2yaLPxsjsbBSkGZvlQcGB7IWyF6wf\n+V96cet5unjxvW9mMjJlmYJfhHPuIoCLAHDb919wOBlE5h44peRGptJE74XEtylhkitlntT9ip1r\nado2hbvNS0LTDjN5yr0kSp/N2DOaKvC5tDYcifjY/nPhhxrmn43JltloJ/hvXwqa9gB12hTmXNqU\nORUv+KVuu2Wcey8CeEewfocPI4Qcc5ZR/K8AeJeI3IWqwH8cwCeyW/QAnET6xQc0m/rjwjgbn7LE\nFlL+TFyTiofnl0rTxlqwlFzTlBkPzNQ/lSa0DlJVhRLz/TCRJnZ/26h3alt7/DB8lFhqHkeh6qrC\nq/LqRdALMzbh4f8mNQ9IWlNqlcT2H26YZ+GC75w7FJF/DeBP/ZH/s3Pu64vujxCyPpaq4zvn/hjA\nH68oL4SQNXHkzr0afQCnkfcmN5n6bZw/Jaa+3aaENqZ+zJxGIk3qupQ4QdukyR1vaMLsej+TVrEm\n+CgSNzFxOVM/5QDMmfipZ8MeN/yfNPWDtPsapma6Nf31AC0cwSXOVSV3nUYohl12Cekg61f8MyhT\nQaXEYbdIu3GKEuW3+1hVO3hK6ds47GJpUmkHZh1oVvxhJq2SUvMwzN6zXNom516b5kK77/D/fmI9\nLCU9EzbnAIzciCbrKab4pdfWhhVKORWfkA7Cgk9IB2HBJ6SDrLeOPwZwGfTqI5GGXv0byKtvD5zx\n6mvQPuochVe/cGwCFZ+QDrJ+xb8CdtlFQxp22a2HbbzLbmT/yZF8LbrsTnxLgLUEgiRFg/MWgIpP\nSAdZr+JPAISjhTcyLDexTQlthuNzWO4NOCw3tb9wDofUWP0VD8udGKtgLi+pYbllpgAVn5AOwoJP\nSAdZv3OvaWKgJufe0hzx1FtTE00DOPUWp96yF+ropt6iqU8ISbJexccEwDVkZWpuXrucF87G5dIu\nYzq08QSmzq3Ek5nYNjfX3/S0St7hC1gfJU2vKSstp8hLWXI5q63JExgLt5k6MOElvYpKmvOUnPe2\nybPb1Gup7Fml4hPSQdas+NqDJ0cbxW9Ks4isLNLOF9I0AcMiFfaCSR1apW3R33eq3qn6aWz7EtUt\nsdYWUe/SuJyK58KtsuccGcoi7bVN9zF1TVnHJ4Qk2IDiv96QZhHFXyStsvJmA0/JO7VUydso/iLb\nl/gf2qRt42tZxi+zjDUYS5PKW85iWYX/h4pPCFkDG1D81xrSLKP4OY5K2UtZ5h27rOIvs99F0qxK\niRdJu8y2q7BGYl79JlalvxOUni8Vn5AOwoJPSAd5mzn33q4clam/Ko7zPTuqvKyy6rhKU5/OPUJI\ngjUrvkPz5z5afA6kM/CavL1ZpVVSNgiNik9IB2HBJ6SDsOAT0kFY8AnpICz4hHQQFnxCOggLPiEd\npLHgi8g7RORJEXlGRL4uIg/68JtF5Msi8pxfnj367BJCVkGJ4h8C+BXn3N0A3g/gF0TkbgAPAXjC\nOfcuAE/4dULIDUBjwXfOveSc+wv//w0AzwK4HcC9AB7xyR4B8JNHlUlCyGppVccXkTsBvAfAUwDO\nOede8lEvAzi30pwRQo6M4oIvIicB/CGAX3LO1WbMdM45JDoJi8gDInJJRC5dv355qcwSQlZDUcEX\nkSGqQv9559wXfPB3ReS8jz8P4JXYts65i865C865Czs7Z1aRZ0LIkpR49QXAwwCedc79bhD1RQD3\n+f/3AXh89dkjhBwFJcNyPwjgkwD+SkS+5sN+HcBvA/gDEbkfwN8C+OmjySIhZNU0Fnzn3P9C+ttL\n96w2O4SQdcCee4R0EBZ8QjoICz4hHYQFn5AOwoJPSAdhwSekg7DgE9JBWPAJ6SAs+IR0EBZ8QjoI\nCz4hHYQFn5AOwoJPSAdhwSekg7DgE9JBWPAJ6SAs+IR0EBZ8QjoICz4hHYQFn5AOwoJPSAdhwSek\ng7DgE9JBWPAJ6SAs+IR0EBZ8QjpIybfzVogA2FnvIYvpLxBX8t6cLJCXVbPp9/ui12C8wjzk9pW6\nv7lnInVNc9usg2FRqk0/EYSQDbBmxe8DuGm9h0zSTyx7LdKUvN1LVGsRRWzzzl5GhRbRBns+sWtQ\nksaGLXMtSxTf3le7jKVJbbMpyixqKj4hHYQFn5AOsmZTfwDg7HoPOYc10YYmfCuSdmjidFtZ4Pgu\nE1di8qfe1YvkZRPo+adM/XEkrKlasGw1Ycsse2Y9NN8Tprw+IpuWUikr0pvOJiFkA2zYubeII2TZ\nJp6Uim/NJ7VGwbZZj73lm8RpEijzXNqC62GT2Fd37FXetE0uraVNC+bIL8NroOc/6Zs0/fq24f+U\n8BeRsjBCzEnrfdXSEfrLBiaNfTY27dvbbk4CUPEJ6STFii8ifQCXALzonPuoiNwF4FEAtwB4GsAn\nnXMHzYe7ZeHMNtOmjuyVx76xwzemDbNpVQlKFD+WtVRc7jR6ZqnElCbV6lhiJaT2m7Mo9HxUxa3y\n5+JK0pZcL5tmbCwMDQ/Pw97nnUR4LCxlDW6Kwv5xbRT/QQDPBuufBvAZ59w7AbwG4P4W+yKEbJAi\nxReROwD8KwD/AcC/FREB8CEAn/BJHgHwmwA+l98RgOFRep/9Wz3nBrBv6BOJ9TBs1y93zHquXjen\nPGbd/k+lAfIqm7IAYmlK1LvJkmij+IdmPfyfStNG8WNWQiqNDQ/PQ+95030O06aen1UpfpMkp6zC\nFdfxfw/ArwaHuwXA6845vW0vALg9tqGIPCAil0Tk0vVrrxYejhBylDQqvoh8FMArzrmnReRH2x7A\nOXcRwEUAuO38BYfTiYSLvuFK0e31jPVNfdKs72KGDUuth2/5UsUpSavk6u+pHqSxsJSat2kJKFF8\nlYPrZh2YqfO+WbfhsbiU0tdaDRrSxBQ/9QycMMswbm6b6kD9YXiyZfT6qxnINRn3gJ2yfZWY+h8E\n8BMi8hFURtBpAJ8FcJOIDLzq3wHgxQXzSwhZM42mvnPuU865O5xzdwL4OIA/c879LIAnAXzMJ7sP\nwONHlktCyEpZpgPPrwF4VER+C8BXATzcuEUfqJn6m+i/o2a5NdVOmiUwb9aZpZyo7NJBYN5NxtW7\ndDKpls6vw69jFJx0k9NK6UX+zznufEeVXrCx/y/elOz5dWta9oJtwv9hWuk5f7j5G9D3GR77zIxG\nVWeog/1qOR4Fj9m+vwHW1N83yzAuZfLb9ViYrX7oevjs7fll4j7XTP2T1Q62T16torar5S6uAQAG\nKDf1e0cwV8N2r6FF3dOq4Dvn/hzAn/v/3wbwvpb5IoQcA9bbZbePoxmj0+bFqYqvlod9u4cWiYkb\nnnkLALC7e9UHV+vhW/7QX9KJV7+J7yh06A/sgsE0qpDjid/GWAtKTZETCqyq2w8UWeN60LST7LJ2\nzII0Mj2mUfyhV3xdBt7Pke8afTCpWwWj677L9LXAU5pyAOace7Z5sETx7bMwXa92snfmzWnS3V6l\n7Cegz0AVp4q/hbTiHoXCW3LHD2GXXUI6yPoV/0xDGvsqavOSLKn/awcHzcdpswzy1z9dvcX3TlZv\ndVV4fcvrMlTZsZeSqZobC2AcnOA0rleFHfbqaZVQKawCq7Vh1TfMl7UKJGEBxMLaWAWa7+u+B8yB\nqnswAGrf34CDXhV2fden3fVpzwZpr1VpD72PwKmvQP0kthMQ0NyJSJdhE6xaoaerjXdPV/d1b1i/\n3+H/PfMs6DoVnxBybFmv4g+QnnKvycOfU/M206xpd0xV+LP19e2zb0yT7m3X3+qncaUK9293XQ8V\nX+v0blrH92qOeTWfFKQB2ilyLq3S9zI4q/uP57YZTK2F9P4H0/2kFH/ol6Hi162BqQXg168Ho0ym\nVsBu3XI4VKvK+0bUTwAE1oGGjUQP7Nf9MlR8r/R7Z/397en9rp6FvUDxZ2H1Z+OUD1fFzal7rGVk\nPk1uwpbZc2YZ1Pov5/ZPCOkcm/HqlwwoUVLdWMO4Nmm1jp9QelX5KqpSAH2bz97uV3x8/S1fHaau\n4jY8F5batg0l9UjrHwhbJfS/WjFDryC6HkurS823rdvvByNHbFybtNaS2O/V/QNA4DPQ/Xk/wVyr\nwXCmqNomP/PYV0v11Ktvpwqre/PtMqb49p7k1mXOOquvjyPPRmhd9oscXVR8QjrJ+uv44TwcsenK\nU4pfoua5HnAap4JilP7Udl3VgZnSW+W3df2d6WiUxUjV+SYRx4eqqvYHGE/f9v1aeGwbXarCDL06\nDYLjq2JtTeMOa2m3gjqk1ie3TL1SFXkU8epbFdf6ei6tKv1V34VOrQJdvx5YCdd82DTOWwDXtqtu\nmFf7VXjY21J739m2eV3XZRX3Vi1szyh+7FkQ40uZ+UvqrSv1OL+c1BVfW4BSvqIBFZ8QkoIFn5AO\nsgHn3nhu8Agw65Y6MOOZtXlmbtALMBv4osvp/Go+PrRAjXNPu9/aJjs154G0iW+de6F5pyZayklT\nd+TUm2ysKTgxZjww6yBkm/50/TC4pbM43WZY27+a82H+hyZMzfitiQ+/NjMlRZvIrqHOto/wWRkH\ns8IcDqp7ZLvzHk6b/ua796qp/6YfTaPm/Ju+j+1b01E2wFWfmW3ffqdxvV6Vbzk930y2O3Xm1Qfc\n2O64QGji63Pjn6OpqV8dt2a+e3O9N/bP+KE38X0SKZlPQBn6xJGBW64P9E3VIAUVn5AOslbF7w8O\ncfKWy3NdSKv/9e6lymRbu8DWu8ICMyUc+y6cahVYKwGYdffsb1fKdWo377gLw27C69m0oSLMdakd\n+U4uk+otPwgMGrHOSDuQRAkUU9XTKqcqfd05NvS7q8dp3ra9qoeKr4p14lq17Oup2WUsTM/DfLKg\nH3SW6Q+q67CtVsGwbh2EnzcY++2u7lYn/QZOAZgpva5fCUZWXfFhM6fkqHbOMWYqXnfm2aa7Kk28\nq+7JiXf0eouopuJ6X1P3eVSQVtHr1JsPk2EkfQIqPiEdZK2K38MEp3FlrqtnlRHtOFJ/ZY1N3TVs\nxpjWa4de8dUC2NYmopnUOP8Fl36v2r/teGHVHEgr/RlcrsInXvHfDF7vdlBIar0krRLMA9j3it/f\n9cq55RVzt1qOgg5I17e1O2y9eW22W99hZX8m40PdXJfXzPps97P/KohafR6YZdg91obZtIF1o5bC\nKT/hyambq+Ubp9/w+b/qN5nN3rFlmij1GRsbX0jIrumwM70upu5fhV2NLnev+GdAjaeYitshwrFn\nwoalJmUJTyO8llR8QkiK9dbxMcYpXMEwou6DSJdQIPRWax1/luWR/2+HvsY83Ie9anv1Ulsv/p5R\n/vB/Uun/wb/CZ1VATHvvqggdmvVw1KSddspuq8782Cyvutyrrw8D62C4560Abw2oBaCcfKsKlzD/\nb5ilqrpelqtB2ismjZ6P5ruN4ueU7FQ9T6duq/K9c/YVAMDWdtipaN6aBMJh0fOWY0rFZx16ZmbO\nnq3b+2s4ZxHlZhY+NMuYdRCzBqoTrEhd08Kp6aj4hHSQtSv+TbgcHRyidTM7yEC9+bMunoGKG2XX\nNLatO/yv7dTWO6tt8qciXv2k0v+DTzhzC8xPGql1ZKvmuTgNjym+OrD3GpbBfqeO85PhwQF5PZJ/\nPf3LZt1aApE0+nkVseod9jy2Cm+VP5ZWz8n4HYa+Pn3ruddm23iLx1p/tvtvTPFndft6d9wTkTr+\nyWtVWklZSOGltlafVfOcFZhS/NB4C1tRWMcnhKRgwSekg6zd1D+NK9lRYSnnnh3NBcyap+bTDHx8\n2P1zWDuOddzNOuXMd9lNmvgxUz/V4SXWAeZ6PM5bkej71/JWOPPvKbM8bdbDOQ2No0i0uU0vu5rq\nr882mf63cZG0b/qwv5/UDpP9VufQhOly6BP3AymaO39bjfLn1w964d76/ZXZ73Z1duPqCGriX/N1\ngbAauGuceXvTEXh+zsVwzr1RFde3Tk/r6AzNd+u8TTl1w7BRJA6YdXAKnXvbQRxNfUJIirUr/hlc\njqp7bLw3MN/5JHxTp2ZxsQ7B8P9WwrlnO/IAMytg93Uvkap2OcVv0QFGlf3KYX03mkTV8Obvzba5\n1WdvS+cuVKsh5gxKdQHWli491VDx1U+m5+ZV3fn1V4L867eP/97k2yp9TvGnaSf1ZZjmJn/+P2Sd\nYpFp6fp+nNaZH6wyfr2nY/n3/LJS/LAzk51xZ75Zb2ambb/hD2qdevbmhZaddd7aZc46qPtj6+oe\nC6PiE0JSrFXxBQ5bOJib5QVIK74d0HNgup1WaeoDYw5qFaA69thDsx42J+oAm7n6lu2AkeuGa97c\nB8Eb/LpPq6KtS6ucs3lngbf8/rZUUawChM08NswOn7XNiUE+bT10NKoHh5vpcmyWsUmT7IdsUl/j\nDuP0/A98fresvyRURZ8ZHSyztVe/v/b5qsfVLdHpwLFwqGtuJqiQktmlNsgxygohZF2sdyKOAhaZ\nYXaZbXRgz6xL5+xVfTisLs/20KuEKqjtNhub8MOiHupACdQzrENfpx5us2n4KYLT9thW1cM7mros\nmsfYzMbbZuk7z2z5c7wlPL/DehasQRFT9ZTCpywAYHb+U7+GtmBox6aIlXNonuz5LxClJdvOa6hf\nOAIAt+uH3Vqfij4DekKhFaUmi26j98527An/x+KA+H0O71na2K1BxSekg2xU8cNJNfrTd72ZiCMy\nAccqmb7VpwN+5of9Oj8IRGx9Oqb4is1uRF21nfq0vwtD7Yp6WN/kdODW6JtBOdNlbPBG6XcIB5H/\nVpUm9WAA+D5vsQy9ulnRm+Y5csiUwteqxn5HJ/Wcbd8F/bptmCl//jpU2z439hsAVX7jA3p0EFit\nJWmrOtltPyx6bqosvf6xgTcpKyH0EaXSKLaLM1C3AgpLNBWfkA6yEcXXN2pMCZrq67n4XFzqrW6/\nYx8O+51OUqmKrMqS6omVI+bZNYNZdv1yy0zN3g8H3uh/rTfaOnk4rb61MlKTOoQ3QvdjJ4KI5H/L\npz3nlX/st+mZ2yCx22LDcl9X0nPWOr7W7a3VA0zzP7GZmB6m/hWhMEyxQ3jDliSdDHbghzr37TXV\nxyf2Bd+U0seG8CYn25xmej6Mik8IyVFU8EXkJhF5TES+ISLPisgHRORmEfmyiDznl2eb90QIOQ6U\nmvqfBfAnzrmPicgWKmPr1wE84Zz7bRF5CMBDAH5t0YykzPRlPiDZ5rjqRKyP4fdzvW9Xy+G+t7vU\nHFYTM/RHWq+VnY0m5kgzY6z7tl1sN/J/yyxLnHspYnnSqoQ18cO0xsnZPzRpS/JR4glU014HINk5\nCWLNeeqYnX42rD7DU8lzFWvi1c5hvd3KFt/Rj4rq7vSahOZ7anbd2Lx6ubjqRCrC+xBW6Zo+N292\nk0REzgD45wAeBgDn3IFz7nUA9wJ4xCd7BMBPlh2SELJpShT/LlTjMX5fRH4EwNMAHgRwzjn3kk/z\nMoBzJQdcpYIv13GnvozN2qNv9+kAH9+sN7RNXbG+ICmljI1YsV1qVTWcWQ/TLtKBx+Yz5nDcNmG2\nV1HYoSTR5DdHG8WPocexQ5Jt0yZmc/GnPjmec+5p025q9p4Yk90q7dbAd/c1X8kBIt9PyH0IVu/5\nKBIHxHt5SRC3wg48AwDvBfA559x7UHUlfyhM4JxziI6VAkTkARG5JCKXLr/6RiwJIWTNlCj+CwBe\ncM495dcfQ1Xwvysi551zL4nIeQCvxDZ2zl0EcBEA3n3hrtrLof4Z6LEPm//M83za1TFT/HmFmM3p\n74f52rq+7cgDzN7IqoxW8cM3slXrmKqG24ZprMLHLIpUBx4bHubJNgHaDinhiKFUk1OOptsYkyI9\nV6v0tmkTwMG2Wm71A9lPUoeKb79hODGdunJf4Zn6gYbVcjAcJ7dJffpav6lXC5vMWw5A+LnIyOfQ\n+z1MVtWc55x7GcDzIvJuH3QPgGcAfBHAfT7sPgCPlx2SELJpSr36/wbA571H/9sAfg7VS+MPROR+\nAH8L4KdLDzrrwDOrwLiE0tvwVfkIrGUx8+oHg3SmU3jpRB9+qKfW9WOTHljFtLOihmquin+QWNfL\nE/PgpmarjY1rtaSUPwwbmLS7kbRlH2aN0+Y26nXRLrqmA0/4Nd6Dnk7GMj9/PhB+y2F2Uee+1zgd\nuFWpbljHt/X/2RDeoV82X5SelfHItbBWiCVVXsaFRboolXPuawAuRKLuKToKIeRYcQyH5bavyy9j\nBcx79WcV3kNfeZ2bm31bv4Dru22GO7R1bjtAIza4QuNsG3BMPFKTOsS635bW8WMWRdPxUsc8Cuz8\n+qr0fnl9N7xn9WHWlpkip+eosqoeWgSpVoKcH6ANizzL4bGtbyO9DSGkc7DgE9JBNmrqpxwUy6Yt\nIdWRJ+bcUxNfpk1B9W6bW71Ze5Z+rmquGSz26SONs82Cuo9lrUe7fWokWeyzVW0+annUT5Hmz1+n\nkb+G9jPgwPxszHpf1Ry2n2iL4aYjNfu15aLYOQG0Oht7pq2pn6papMJKnXtUfEI6yFoVf4Ie3sRe\n9JPY5V/SiYyXn869P6yliX00U9+OqW6ZO0HTjarHvpfpbd9TZN8vt/28uLXPNPv/eo5bE39eI3/O\nwemJHZdtZ7i1X76J0aTqsTCr3jkV9+rqdGabIK3OcqNNaFZdV4W1xlJfUKqH6bNQ73Yd65qdOl6b\nuJxSp+JindXsoLEUocMxvN4jKj4hJMVaFX+EIV7F90U/kz3rCFFX/LFR75iKj4zi5z6TrW9HtTB2\nvGoPvczuYDb9zVTRp2n3a+vbZr06j7riD3r+XLfry3C7/vR61C2hVBfPkFh3z2ncJN4JRLt76izC\nOSvKXsvYNVU1TXW3XpaJqWvn7rO1DFPnk1PkpvBYXNP6ommsHyDWsaeu+GWjdKj4hHSQtSr+IQb4\nLs5F1X3YoPhadxlHFCe1DL2x+ibUupF+UWVe+WfqbdOk0tYtl3EtrbVuwrTzSm+sBU1bMKy1pF6d\n8m/EVDylrrH6tPWgL0LOWrD5dQX5t3k6nFoN6bymZnJ2BcrcFB6LW3T+SCB9v6n4hJAk66/jT25D\nv1dXuur/aC4MmK+jhe3sYz8G8XBUX479cjqGEQBGxkuqA212vJpv+/p2L6yDxxXfLvsRxU8pfUzx\nZ9ZBvI6/KKnt84pfrzeX+FhmStxcx2/6PkLMm52aIMOuh2HzaXWo7TCZ11Tech52N0kMIx8XTO01\naZ9mboCPYeSo+ISQBOttxz/o460XzwYfSQ88lL6Bu2/Guk7VWxuQx8Eb1raDj8wyfDnaySL8pBoj\nP4HCaNuPAAlfmNv+O2k+T2oV6Nzqah2EX2BtUvqcX2PVip9iXvEDKyqhpnnFV095XBlz9dWclTCb\nHMX3dPPqqmqqahiqoo1zqrz2+cld2mX7TTTR3HlwYdw+B+kQQhKw4BPSQdY7SGcE4P8hmDUmMPP6\nvtloYJwTdibb2CepUyZ+mNaa+jpPmx2UEh7ed0l1frk/qPqv7tu0tW189cVUXQZmCWDOyWkdnIuY\n+m3mi481w02bvYzjVE1nXQcAp//VcWqzGzNpU6cUC7fb21lp7Wy1sW1T25TkqU3+j9B8b8VBcxKA\nik9IJ1mv4h8AeB75D/9Z34RV+pjDzip8TPFTA1Wsaufmt7Np7T6AmRXjnYZjXfp97NfS2mXdWiii\noEloLq11cMUsoyZrqikOyCvmItZBygJYdpvSbUv2o9ssOpJ3ESkO80LFJ4Sk2HAdP4iz6qqU1PGt\nOtnvj4VhqePZ2XHD/Nm55bKKn9hvbF79OcX3Sqx+Dt12EeWJxdn92ObQME1K+UvStlHznCI3uTiW\nqVeX+BRK8tHGDZOS2Zz8pizgVF7sNxkWOCQh5G3KZhV/2Tp+Sunb1PGt0sfmpW9KG/seXsovEFP8\nlL9Bw5dVp5SqtvGb5KyoNorfpPSxL8emWKQenbumJcddRZ8qK7ex82iS5NR9Zh2fEJJivYp/COC1\nMVY3Efu4YZlxd05s23Oskm/jbL5j25guqG38AdbqaaP4JW3ZNm3si6w5a2CO1KddU43osczl7lnK\nMaNJc/eqdD2Vv1R4qamVydvEnkcsT6nnsemalrUIUfEJ6SBrnl77EMBriCto6rMsOfVIqUZMrlJv\n6tSnY3JpbNowfqseNhrWw/dbWAc5xS/xijeiih27TgcmzoaHYanucfa+5NLY+FickrLAwrCmivT8\nl5qbjwuUd93LfdIol5em5zFn2lHxCSEZWPAJ6SBrNvUnAK4jbiKnzLeUOa/7i6UtqRakesfEzMeU\neRezxXV/W2Y91pdT0/j9jFY9S62dkTVlgufa2+w3u2Pn2rTfJvPU5sGmSZHrH2vjNA89Ex+yyAic\nkl5EbfJiy0GJ4zH8n/+8tkLFJ6SDrFnxHSrHW04p2zj3mhxEOesg1fwSe8OmnHv2DR7uR8PUYxc7\nPxtm+wTHaOoPW6KcJc63knFxEVklAAAHQElEQVStpQqfc9jlFH+Z6W2sYpb0f07tq22cjW/KS/hM\n5JrrQlJWFBWfEJJgA4p/gLhSKk3NFzl1stu0aRrKKX2bup/W220eYs1MWn82TYDZfpdt8tJUPyyp\nt6e2iaVto+Yl92wZStS1KU9t0th6e5u8lFgHueOzjk8IKaBI8UXklwH8PKrXyV8B+DkA5wE8CuAW\nAE8D+KRzrmGIgNbxtYNNbERMihIPsV3PvR1L6nxNecq1ANiWi1ynJXs9Sro0NylPyTZtutQu4pkv\nqben8tSGmGLa/eV8OClK8pI655JnI+f3QSQuFm/jVqT4InI7gF8EcME598M+Jx8H8GkAn3HOvRNV\nd7z7i45ICNk4pXX8AYBdERkBOAHgJQAfAvAJH/8IgN8E8Ln8brSOr4Rvs1EkDFjMa51TKTTEhe/C\nJnXIdRm1/oGclZPqw1AyhlRZxAO9aitKWWYOq0XaxWP7s5pW0ubfhqZ7EmvpSeUl5+taxLJrplHx\nnXMvAvgdAH+HqsBfRmXav+6c047BLwC4Pba9iDwgIpdE5NL1668tlVlCyGooMfXPArgXwF0AfgDA\nHoAPlx7AOXfROXfBOXdhZ+fswhklhKyOElP/xwB8xzn3KgCIyBcAfBDATSIy8Kp/B4AXm3dlu+yG\nlDr37P6WTZsyw0ooMZFLRpLZ9dz7uI2J16aTSSqsxPm2iIOupCvqMqRM+1Vev6PMyyJT8DTts3zv\nQGXiv19EToiIALgHwDMAngTwMZ/mPgCPFx2RELJxGhXfOfeUiDwG4C9QDfb9KoCLAP4IwKMi8ls+\n7OHmw2lznhK+d+ynblIs6+haJq2l5I2dclqGaSxtZihatWNqFc2EbeNL06S2WbZTzrpo6kRWkrZp\n/2XNeUVefefcbwD4DRP8bQDva5UvQsixYENddnO06bCQYl1v+RJl1jQ5i2ZVcxCmOOpreFRW0zL7\nOOpOqauYUF9Z5fPKLruEkAQbGpabo7Sun2Ndil/y1l/F+Rx3jlM9WjlOeVpnXqj4hJAEa1Z84Hi9\niZfl7XQupEtQ8QnpICz4hHQQFnxCOggLPiEdhAWfkA7Cgk9IB2HBJ6SDsOAT0kFY8AnpICz4hHQQ\nFnxCOggLPiEdhAWfkA7Cgk9IB2HBJ6SDsOAT0kFY8AnpICz4hHQQFnxCOggLPiEdhAWfkA7Cgk9I\nB2HBJ6SDsOAT0kFY8AnpICz4hHQQFnxCOggLPiEdhAWfkA7Cgk9IB2HBJ6SDsOAT0kHEObe+g4m8\nCuAtAN9b20GX41bcOHkFbqz83kh5BW6c/P6Qc+62pkRrLfgAICKXnHMX1nrQBbmR8grcWPm9kfIK\n3Hj5bYKmPiEdhAWfkA6yiYJ/cQPHXJQbKa/AjZXfGymvwI2X3yxrr+MTQjYPTX1COsjaCr6IfFhE\nviki3xKRh9Z13FJE5B0i8qSIPCMiXxeRB334zSLyZRF5zi/Pbjqvioj0ReSrIvIlv36XiDzlr/F/\nE5GtTedREZGbROQxEfmGiDwrIh84rtdWRH7ZPwN/LSL/VUR2jvO1XYS1FHwR6QP4jwD+JYC7AfyM\niNy9jmO34BDArzjn7gbwfgC/4PP4EIAnnHPvAvCEXz8uPAjg2WD90wA+45x7J4DXANy/kVzF+SyA\nP3HO/WMAP4Iq38fu2orI7QB+EcAF59wPA+gD+DiO97Vtj3PuyH8APgDgT4P1TwH41DqOvUSeHwfw\n4wC+CeC8DzsP4JubzpvPyx2oCsuHAHwJgKDqYDKIXfMN5/UMgO/A+5SC8GN3bQHcDuB5ADcDGPhr\n+y+O67Vd9LcuU18vpvKCDzuWiMidAN4D4CkA55xzL/molwGc21C2LL8H4FcBTPz6LQBed84d+vXj\ndI3vAvAqgN/3VZP/JCJ7OIbX1jn3IoDfAfB3AF4CcBnA0zi+13Yh6NwziMhJAH8I4Jecc1fCOFe9\n7jfeDCIiHwXwinPu6U3npZABgPcC+Jxz7j2oum3XzPpjdG3PArgX1cvqBwDsAfjwRjN1BKyr4L8I\n4B3B+h0+7FghIkNUhf7zzrkv+ODvish5H38ewCubyl/ABwH8hIj8XwCPojL3PwvgJhEZ+DTH6Rq/\nAOAF59xTfv0xVC+C43htfwzAd5xzrzrnRgC+gOp6H9druxDrKvhfAfAu7xndQuUs+eKajl2EiAiA\nhwE865z73SDqiwDu8//vQ1X33yjOuU855+5wzt2J6lr+mXPuZwE8CeBjPtmxyCsAOOdeBvC8iLzb\nB90D4Bkcw2uLysR/v4ic8M+E5vVYXtuFWaPT5CMA/gbA/wHw7zbt3Ijk75+hMjX/EsDX/O8jqOrO\nTwB4DsD/BHDzpvNq8v2jAL7k//8jAP8bwLcA/HcA25vOX5DPfwLgkr++/wPA2eN6bQH8ewDfAPDX\nAP4LgO3jfG0X+bHnHiEdhM49QjoICz4hHYQFn5AOwoJPSAdhwSekg7DgE9JBWPAJ6SAs+IR0kP8P\ns4hWcHfsf9MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meb_RIZVdHlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}